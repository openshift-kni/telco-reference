---
apiVersion: policy.open-cluster-management.io/v1
kind: PolicyGenerator
metadata:
  name: core-overlay-18
policyDefaults:
  namespace: ztp-core-policies
  policySets: []
  placement:
    clusterSelectors:
      common: "core"
      version: "4.18"
  remediationAction: "inform"
policies:
  # Baseline configuration with custom overlay content
  - name: core-overlay-base-4.18
    policyAnnotations:
      ran.openshift.io/ztp-deploy-wave: "2"
    manifests:
      # ICSP/IDMS/ITMS content will be entirely custom, thus including that in this policy
      - path: reference-crs/required/other/icsp.yaml
        patches:
        - spec:
            repositoryDigestMirrors:
            - mirrors:
              - example.com/sample-path
              source: nomatch.io/sample-path
      - path: reference-crs/optional/other/sctp_module_mc.yaml
      - path: reference-crs/optional/other/worker-load-kernel-modules.yaml
      - path: reference-crs/optional/other/control-plane-load-kernel-modules.yaml
      - path: reference-crs/optional/networking/multus/tap_cni/mc_rootless_pods_selinux.yaml

  # Custom networking, operator and cluster configuration
  - name: core-overlay-config-4.18
    policyAnnotations:
      ran.openshift.io/ztp-deploy-wave: "8"
    manifests:
      # Set auto sizing of control plane nodes
      - path: reference-crs/optional/tuning/control-plane-system-reserved.yaml

      # Multus networks
      - path: reference-crs/required/networking/Network.yaml
        patches:
        - spec:
            additionalNetworks: []
      # - path: reference-crs/optional/networking/networkAttachmentDefinition.yaml

      # Numa aware scheduler, binding to mcp
      - path: reference-crs/required/scheduling/nrop.yaml
        patches:
        - spec:
            nodeGroups:
            - config:
                # Periodic is the default setting
                infoRefreshMode: Periodic
              machineConfigPoolSelector:
                matchLabels:
                  # This label must match the pool(s) you want to run NUMA-aligned workloads
                  machineconfiguration.openshift.io/role: worker-2

      # MCP PerformanceProfiles
      - path: reference-crs/required/performance/PerformanceProfile.yaml
        patches:
        - metadata:
            name: worker-profile-1
          spec:
            cpu:
              reserved: '{{hub fromConfigMap "" "hw-types" "role-worker-1-reserved" | toLiteral hub}}'
              isolated: '{{hub fromConfigMap "" "hw-types" "role-worker-1-isolated" | toLiteral hub}}'
            hugepages:
              pages:
              - count: '{{hub fromConfigMap "" "hw-types" "role-worker-1-hugepg-cnt" | toInt hub}}'
                size: 1G
            nodeSelector:
              node-role.kubernetes.io/worker-1: ""

      - path: reference-crs/required/performance/PerformanceProfile.yaml
        patches:
        - metadata:
            name: worker-profile-2
          spec:
            cpu:
              reserved: '{{hub fromConfigMap "" "hw-types" "role-worker-2-reserved" | toLiteral hub}}'
              isolated: '{{hub fromConfigMap "" "hw-types" "role-worker-2-isolated" | toLiteral hub}}'
            hugepages:
              pages:
              - count: '{{hub fromConfigMap "" "hw-types" "role-worker-2-hugepg-cnt" | toInt hub}}'
                size: 1G
            nodeSelector:
              node-role.kubernetes.io/worker-2: ""

      - path: reference-crs/required/performance/PerformanceProfile.yaml
        patches:
        - metadata:
            name: worker-profile-3
          spec:
            cpu:
              reserved: '{{hub fromConfigMap "" "hw-types" "role-worker-3-reserved" | toLiteral hub}}'
              isolated: '{{hub fromConfigMap "" "hw-types" "role-worker-3-isolated" | toLiteral hub}}'
            hugepages:
              pages:
              - count: '{{hub fromConfigMap "" "hw-types" "role-worker-3-hugepg-cnt" | toInt hub}}'
                size: 1G
            nodeSelector:
              node-role.kubernetes.io/worker-3: ""

      # Cluster Logging
      - path: reference-crs/optional/logging/ClusterLogForwarder.yaml
        patches:
        - spec:
            outputs:
            - type: "kafka"
              name: kafka-open
              kafka:
                url: '{{hub fromConfigMap "" "regional" (printf "%s-log-url" (index .ManagedClusterLabels "region")) | toLiteral hub}}'
            filters:
            - name: custom-labels
              type: openshiftLabels
              openshiftLabels:
                sitename: '{{hub fromConfigMap "" .ManagedClusterName "logging-name" | toLiteral hub}}'
                siteuuid: '{{hub fromConfigMap "" .ManagedClusterName "logging-uuid" | toLiteral hub}}'
                label3: 'other data'
            pipelines:
            - name: all-to-default
              inputRefs:
              - audit
              - infrastructure
              filterRefs:
              - custom-labels
              outputRefs:
              - kafka-open

      # Metal LB
      - path: reference-crs/required/networking/metallb/metallb.yaml
        patches:
        - spec:
            nodeSelector:
              node-role.kubernetes.io/worker: ""

      # - path: reference-crs/required/networking/metallb/bfd-profile.yaml
      # - path: reference-crs/required/networking/metallb/addr-pool.yaml
      #   patches:
      #   - metadata:
      #       name: pool1
      #       annotations:
      #         metallb.universe.tf/address-pool: pool1
      #     spec:
      #       addresses:
      #       - 1.2.3.0/24
      #       #bgpAdvertisements:
      #       #- aggregationLength: 32
      #       #  communities:
      #       #  - 65535:65282
      # - path: reference-crs/required/networking/metallb/bgp-peer.yaml
      #   patches:
      #   - metadata:
      #       name: peer1
      #     spec:
      #       # Dummy address for now. One of our node ips
      #       peerAddress: 1.2.3.9
      #       peerASN: 64501
      #       myASN: 64500
      #       routerID: 10.10.10.10
      # - path: reference-crs/required/networking/metallb/bgp-advr.yaml
      #   patches:
      #   - metadata:
      #       name: bgpadv-1
      #     spec:
      #       ipAddressPools:
      #       - pool1
      #       peers:
      #       - peer1
      #       communities:
      #       - 65535:65282
      # - path: reference-crs/required/networking/metallb/service.yaml
      #   patches:
      #   - metadata:
      #       name: svc1
      #       namespace: default
      #       annotations:
      #         metallb.universe.tf/address-pool: pool1
      #     spec:
      #       selector:
      #         app: lbapp
      #       # Dummy IP for now. Borrow cnfdf03 api IP
      #       loadBalancerIP: 1.2.3.5

      - path: reference-crs/required/networking/sriov/sriovNetworkNodePolicy.yaml
        patches:
        - metadata:
            name: test-signaling-snnp
          spec:
            deviceType: netdevice
            nicSelector:
              pfNames:
                - '{{hub fromConfigMap "" "hw-types" "nic-config-type-1-sriov-dev1" | toLiteral hub}}'
                - '{{hub fromConfigMap "" "hw-types" "nic-config-type-1-sriov-dev2" | toLiteral hub}}'
            nodeSelector:
              nic-config: "type-1"
            numVfs: '{{hub fromConfigMap "" "hw-types" "nic-config-type-1-sriov-numvf" | toInt hub}}'
            priority: 99
            # excludeTopology: true
            resourceName: testSignaling
      - path: reference-crs/required/networking/sriov/sriovNetwork.yaml
        patches:
        - metadata:
            name: sriov-network-signal-1
          spec:
            capabilities: "{}"
            ipam: '{}'
            networkNamespace: signal-1
            resourceName: testSignaling

      #
      # Configure ODF
      #
      - path: reference-crs/required/storage/odf-external/01-rook-ceph-external-cluster-details.secret.yaml
        patches:
          - data:
              # encoded content has been made generic
              external_cluster_details: eyJuYW1lIjoicm9vay1jZXBoLW1vbi1lbmRwb2ludHMiLCJraW5kIjoiQ29uZmlnTWFwIiwiZGF0YSI6eyJkYXRhIjoiY2VwaG5vZGUxPTEuMi4zLjQ6Njc4OSIsIm1heE1vbklkIjoiMCIsIm1hcHBpbmciOiJ7fSJ9fSx7Im5hbWUiOiJyb29rLWNlcGgtbW9uIiwia2luZCI6IlNlY3JldCIsImRhdGEiOnsiYWRtaW4tc2VjcmV0IjoiYWRtaW4tc2VjcmV0IiwiZnNpZCI6IjExMTExMTExLTExMTEtMTExMS0xMTExLTExMTExMTExMTExMSIsIm1vbi1zZWNyZXQiOiJtb24tc2VjcmV0In19LHsibmFtZSI6InJvb2stY2VwaC1vcGVyYXRvci1jcmVkcyIsImtpbmQiOiJTZWNyZXQiLCJkYXRhIjp7InVzZXJJRCI6ImNsaWVudC5oZWFsdGhjaGVja2VyIiwidXNlcktleSI6ImMyVmpjbVYwIn19LHsibmFtZSI6Im1vbml0b3JpbmctZW5kcG9pbnQiLCJraW5kIjoiQ2VwaENsdXN0ZXIiLCJkYXRhIjp7Ik1vbml0b3JpbmdFbmRwb2ludCI6IjEuMi4zLjQsMS4yLjMuMywxLjIuMy4yIiwiTW9uaXRvcmluZ1BvcnQiOiI5MjgzIn19LHsibmFtZSI6ImNlcGgtcmJkIiwia2luZCI6IlN0b3JhZ2VDbGFzcyIsImRhdGEiOnsicG9vbCI6Im9kZl9wb29sIn19LHsibmFtZSI6InJvb2stY3NpLXJiZC1ub2RlIiwia2luZCI6IlNlY3JldCIsImRhdGEiOnsidXNlcklEIjoiY3NpLXJiZC1ub2RlIiwidXNlcktleSI6IiJ9fSx7Im5hbWUiOiJyb29rLWNzaS1yYmQtcHJvdmlzaW9uZXIiLCJraW5kIjoiU2VjcmV0IiwiZGF0YSI6eyJ1c2VySUQiOiJjc2ktcmJkLXByb3Zpc2lvbmVyIiwidXNlcktleSI6ImMyVmpjbVYwIn19LHsibmFtZSI6InJvb2stY3NpLWNlcGhmcy1wcm92aXNpb25lciIsImtpbmQiOiJTZWNyZXQiLCJkYXRhIjp7ImFkbWluSUQiOiJjc2ktY2VwaGZzLXByb3Zpc2lvbmVyIiwiYWRtaW5LZXkiOiIifX0seyJuYW1lIjoicm9vay1jc2ktY2VwaGZzLW5vZGUiLCJraW5kIjoiU2VjcmV0IiwiZGF0YSI6eyJhZG1pbklEIjoiY3NpLWNlcGhmcy1ub2RlIiwiYWRtaW5LZXkiOiJjMlZqY21WMCJ9fSx7Im5hbWUiOiJjZXBoZnMiLCJraW5kIjoiU3RvcmFnZUNsYXNzIiwiZGF0YSI6eyJmc05hbWUiOiJjZXBoZnMiLCJwb29sIjoibWFuaWxhX2RhdGEifX0K

      - path: reference-crs/required/storage/odf-external/02-ocs-external-storagecluster.yaml

  # reference-crs/required/networking/nodeNetworkConfigurationPolicy.yaml

  - name: config-monitoring-4.18
    policyAnnotations:
      ran.openshift.io/ztp-deploy-wave: "10"
    manifests:
      - path: reference-crs/optional/other/monitoring-config-cm.yaml
        complianceType: musthave
        patches:
        - data:
            # The config is a single string value in this map and default
            # content must be copied here in order to add custom content
            # (additionalAlertmanagerConfigs)
            config.yaml: |
              alertmanagerMain:
                volumeClaimTemplate:
                  spec:
                    storageClassName: ocs-external-storagecluster-ceph-rbd
                    resources:
                      requests:
                        storage: 20Gi
              prometheusK8s:
                retention: 15d
                volumeClaimTemplate:
                  spec:
                    storageClassName: ocs-external-storagecluster-ceph-rbd
                    resources:
                      requests:
                        storage: 100Gi
                additionalAlertmanagerConfigs:
                - scheme: "http"
                  pathPrefix: "/"
                  timeout: "30s"
                  apiVersion: "v1"
                  staticConfigs:
                  - "10.11.12.14:9999"

  # When upgrading from Cluster Logging 5.x to 6.0 the old API CRs
  # need to be removed AFTER the new logging is set up. This policy
  # removes those CRs.
  - name: core-overlay-cleanup-4.16
    policyAnnotations:
      ran.openshift.io/ztp-deploy-wave: "9"
    manifests:
      - path: "reference-crs/optional/logging/ClusterLogging5Cleanup.yaml"
